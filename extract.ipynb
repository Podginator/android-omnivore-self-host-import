{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Upload, parse, and store the SQLite File from the Android Application.\n",
    "\n",
    "Retrieve the sqlite database from your android application. This is found in.\n",
    "`/data/data/app.omnivore.omnivore.debug/databases/omnivore-database`\n",
    "\n",
    "You may be able to run `adb root; adb pull /data/data/app.omnivore.omnivore.debug/databases/omnivore-database ./omnivore-database` for this.\n",
    "\n",
    "Place this file at the root of this directory as omnivore-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "con = sqlite3.connect(\"omnivore-database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This will include the /api/graphql - for instance http://localhost:4000/graphql\n",
    "OMNIVORE_API_URL = os.environ.get('OMNIVORE_API_URL')\n",
    "# The API key will have the following format \"00000000-0000-0000-0000-000000000000\"\n",
    "OMNIVORE_API_KEY = os.environ.get('OMNIVORE_API_KEY')\n",
    "SCHEMA_URL = \"https://raw.githubusercontent.com/omnivore-app/omnivore/c9fcbe72ddc6f40dd06e7073b8ffe3c1e71bd650/packages/api/src/generated/schema.graphql\"\n",
    "REQUESTS_SLEEP_TIME = 60 # Number of seconds\n",
    "\n",
    "\n",
    "if not OMNIVORE_API_URL:\n",
    "    OMNIVORE_API_URL=input('Enter your omnivore API URL (Should include /api/graphql)')\n",
    "\n",
    "if not OMNIVORE_API_KEY:\n",
    "    OMNIVORE_API_KEY=input('Enter your omnivore API key (should have a format similar to 00000000-0000-0000-0000-000000000000)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = con.cursor()\n",
    "query = cur.execute('''SELECT si.pageUrlString, si.isArchived, si.savedAt, si.title, (SELECT  group_concat(sil.name, \",\") from SavedItemAndSavedItemLabelCrossRef silcr\n",
    "\tINNER JOIN  SavedItemLabel sil on sil.savedItemLabelId = silcr.savedItemLabelId where si.savedItemId = silcr.savedItemId)\n",
    "FROM SavedItem si\n",
    "GROUP by si.savedItemId\n",
    "ORDER by si.savedAt DESC;\n",
    "''');\n",
    "\n",
    "items = query.fetchall()\n",
    "\n",
    "display(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the articles and tags from the HTML doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "def process_list(item_tuples):\n",
    "    items = [] \n",
    "    for article in item_tuples:\n",
    "        items.append({\n",
    "            'read': article[1],\n",
    "            'time_added': datetime.datetime.fromisoformat(article[2]),\n",
    "            'href': article[0],\n",
    "            'tags': article[4].split(',') if article[4] is not None else [],\n",
    "            'title': article[3],\n",
    "        })\n",
    "\n",
    "    return items\n",
    "\n",
    "articles = process_list(item_tuples=items)\n",
    "labels = set([item for sublist in [article['tags'] for article in articles if len(article['tags']) > 0] for item in sublist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Store the articles and tags in a SQLLite Database\n",
    "\n",
    "We want to be able to track our process, as the API for Omnivore has rate limiting, and takes a while to upload the files. For this we will use a SQL Database.\n",
    "\n",
    "We will store the Articles in the format: \n",
    "- *read*: Boolean on wether the article has been read\n",
    "- *time_added*: The time the item was added\n",
    "- *tags*: An array of strings\n",
    "- *href*: The url\n",
    "- *id*: nullable field that stores the omnivore file.\n",
    "\n",
    "and the tags with:\n",
    "- *name*: Name of the tag \n",
    "- *id*: if the tag has been stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('omnivore.db')\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the Article Table \n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS articles (\n",
    "                    id TEXT nullable,\n",
    "                    read BOOLEAN,\n",
    "                    time_added TEXT,\n",
    "                    tags TEXT, \n",
    "                    href TEXT PRIMARY KEY \n",
    "                )''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS tags (\n",
    "                    id TEXT nullable,\n",
    "                    name TEXT PRIMARY KEY\n",
    "                )''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "insert_tag_sql = f\"\"\"INSERT OR IGNORE into tags (name) values (?)\"\"\"\n",
    "cursor.executemany(insert_tag_sql, [(label,) for label in labels])\n",
    "\n",
    "insert_article_sql = f\"INSERT OR IGNORE into articles (read, time_added, href, tags) values (?,?,?,?)\"\n",
    "article_values = [(article['read'], article['time_added'].isoformat(), article['href'], json.dumps(article['tags'])) for article in articles]\n",
    "cursor.executemany(insert_article_sql, article_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "with requests.get(SCHEMA_URL) as r:\n",
    "    r.raise_for_status()\n",
    "    schema = r.text\n",
    "\n",
    "    assert schema is not None\n",
    "\n",
    "print(schema[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gql import gql, Client\n",
    "from gql.transport.aiohttp import AIOHTTPTransport\n",
    "\n",
    "def create_client():\n",
    "    transport = AIOHTTPTransport(\n",
    "       url=OMNIVORE_API_URL,\n",
    "        headers = {\n",
    "            'authorization': OMNIVORE_API_KEY,\n",
    "        }\n",
    "    )\n",
    "    return Client(transport=transport, schema=schema, fetch_schema_from_transport=False, execute_timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a \"test query\" to check if everything is correct\n",
    "\n",
    "async with create_client() as session: \n",
    "    r = session.execute(gql(\"\"\"\n",
    "    query Viewer {\n",
    "        me {\n",
    "            id\n",
    "            name\n",
    "            profile {\n",
    "                username\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"))\n",
    "\n",
    "    result = await r\n",
    "    USERNAME = result['me']['profile']['username']\n",
    "\n",
    "    print(f\"Hello {result['me']['name']} ({USERNAME})!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_tag(row):  \n",
    "   return { \n",
    "      \"name\": row[1],\n",
    "      \"id\": row[0]\n",
    "   }\n",
    "\n",
    "async def getExistingTags():\n",
    "  async with create_client() as session: \n",
    "    r = session.execute(gql(\"\"\"\n",
    "    query Labels {\n",
    "        labels {\n",
    "              ...on LabelsSuccess { \n",
    "                  labels { name, id }\n",
    "              }\n",
    "          }\n",
    "    }\n",
    "    \"\"\"))\n",
    "\n",
    "    result = await r\n",
    "    return result['labels']['labels']\n",
    "\n",
    "#Then remove all the tags from the ones we created before\n",
    "async def saveTags(tagName): \n",
    "    async with create_client() as client: \n",
    "      mutation = f\"\"\"\n",
    "      mutation {{\n",
    "        createLabel(input: {{color: \"#000\", name: \"{tagName}\" }}) {{\n",
    "          ... on CreateLabelSuccess {{\n",
    "            label {{\n",
    "              id\n",
    "              name\n",
    "              color\n",
    "              description\n",
    "              createdAt\n",
    "            }}\n",
    "          }}\n",
    "          ... on CreateLabelError {{\n",
    "            errorCodes\n",
    "          }}\n",
    "        }}\n",
    "      }}\n",
    "      \"\"\"\n",
    "      r = await client.execute(gql(mutation))\n",
    "      print(r)\n",
    "      return r['createLabel']['label']['id']\n",
    "\n",
    "server_tags = await getExistingTags()\n",
    "all_tags = [row_to_tag(row) for row in cursor.execute('select * from tags').fetchall()]\n",
    "unsaved_tags = list(filter(lambda row: row['id'] is None, all_tags))\n",
    "presaved_tags = list(filter(lambda row: row['id'] is not None, all_tags))\n",
    "\n",
    "tagIds = {f\"{dictionary['name']}\": dictionary[\"id\"] for dictionary in presaved_tags + server_tags}\n",
    "for tagValue in unsaved_tags: \n",
    "    if tagValue[\"name\"] not in tagIds:\n",
    "      try:\n",
    "          tag = tagValue['name']\n",
    "          id = await saveTags(tag)\n",
    "          tagIds[tag] = id\n",
    "      except Exception as e:\n",
    "          print(\"An error occurred:\", e)\n",
    "\n",
    "\n",
    "query = \"UPDATE tags SET id = ? where name = ?\"\n",
    "cursor.executemany(query, [(value, key) for key, value in tagIds.items()])\n",
    "\n",
    "tagIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backoff\n",
    "import asyncio\n",
    "\n",
    "createArticle = gql(\"\"\"\n",
    "  mutation CreateArticleSavingRequest($url: String!) {\n",
    "    createArticleSavingRequest(input: {url: $url}) {\n",
    "      ... on CreateArticleSavingRequestSuccess {\n",
    "        articleSavingRequest {\n",
    "          id\n",
    "          status\n",
    "          slug\n",
    "          createdAt\n",
    "          updatedAt\n",
    "          url\n",
    "          errorCode\n",
    "        }\n",
    "      }\n",
    "      ... on CreateArticleSavingRequestError {\n",
    "        errorCodes\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\"\"\")\n",
    "\n",
    "setLabels = gql(\"\"\"\n",
    "mutation SetLabel($articleId: ID!, $labelIds: [ID!]!) { \n",
    "    setLabels(input: {pageId: $articleId, labelIds: $labelIds}) {\n",
    "        ...on SetLabelsSuccess { \n",
    "            labels { \n",
    "                id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "updatePageSavedDate =  gql(\"\"\"\n",
    "mutation UpdatePageDate($id: ID!, $date: Date!) {\n",
    "    updatePage(input: {pageId: $id, savedAt: $date, publishedAt: $date}) {\n",
    "        ... on UpdatePageSuccess {\n",
    "            updatedPage {\n",
    "                id\n",
    "                savedAt\n",
    "                publishedAt\n",
    "                title\n",
    "            }\n",
    "        }\n",
    "        ...on UpdatePageError {\n",
    "            errorCodes\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "archivePage = gql(\"\"\"\n",
    "mutation ArchivePage($id: ID!) {\n",
    "    setLinkArchived (input: {linkId: $id, archived: true}) {\n",
    "        ... on ArchiveLinkSuccess {\n",
    "            linkId\n",
    "            message\n",
    "        }\n",
    "        ... on ArchiveLinkError {\n",
    "            message\n",
    "            errorCodes\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "createTag = gql(\"\"\"\n",
    "mutation CreateLabel($nam: String!, $col: String, $desc: String) {\n",
    "  createLabel(input: {name: $nam, color: $col, description: $desc}) {\n",
    "    ... on CreateLabelSuccess {\n",
    "      label {\n",
    "        id\n",
    "        name\n",
    "        color\n",
    "        description\n",
    "        createdAt\n",
    "      }\n",
    "    }\n",
    "    ... on CreateLabelError {\n",
    "      errorCodes\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "                \n",
    "@backoff.on_predicate(\n",
    "    backoff.runtime,\n",
    "    predicate=lambda r: isinstance(r, AIOHTTPTransport),\n",
    "    value=lambda r: int(r.response_headers[\"RateLimit-Reset\"]) + 1,\n",
    "    jitter=None,\n",
    ")\n",
    "async def archiveArticle(articleId): \n",
    "  async with create_client() as client: \n",
    "    try: \n",
    "      res = await client.execute(archivePage, variable_values={ 'id': articleId })\n",
    "      # return res\n",
    "      return None;\n",
    "    except Exception as e:\n",
    "          if (hasattr(e, 'code') and e.code == 429): \n",
    "            return session.transport\n",
    "          print(e)\n",
    "\n",
    "@backoff.on_predicate(\n",
    "    backoff.runtime,\n",
    "    predicate=lambda r: isinstance(r, AIOHTTPTransport),\n",
    "    value=lambda r: int(r.response_headers[\"RateLimit-Reset\"]) + 1,\n",
    "    jitter=None,\n",
    ")\n",
    "async def saveLabels(articleId, labels): \n",
    "    async with create_client() as client: \n",
    "      try:\n",
    "        return await client.execute(setLabels, variable_values={'articleId': articleId, 'labelIds': labels})\n",
    "      except Exception as e:\n",
    "          if (hasattr(e, 'code') and e.code == 429): \n",
    "            return session.transport\n",
    "          print(e)\n",
    "\n",
    "@backoff.on_predicate(\n",
    "    backoff.runtime,\n",
    "    predicate=lambda r: isinstance(r, AIOHTTPTransport),\n",
    "    value=lambda r: int(r.response_headers[\"RateLimit-Reset\"]) + 1,\n",
    "    jitter=None,\n",
    ")\n",
    "async def saveArticle(article):\n",
    "    async with create_client() as client: \n",
    "      try: \n",
    "        url = article['href']\n",
    "        tags = article['tags']\n",
    "        # First createArticleSavingRequest\n",
    "        r = await client.execute(createArticle, variable_values={'url': url})\n",
    "        rid = r['createArticleSavingRequest']['articleSavingRequest']['id']\n",
    "        \n",
    "        if len(tags) != 0: \n",
    "            await saveLabels(rid, tags)\n",
    "                            \n",
    "        # Return the article with the id of the saved document\n",
    "        return {**article, 'id': rid }\n",
    "      except Exception as e:\n",
    "          if (hasattr(e, 'code') and e.code == 429): \n",
    "            return session.transport\n",
    "          # I don't know why this happens and I will figure it out later.\n",
    "          print(e)\n",
    "\n",
    "@backoff.on_predicate(\n",
    "    backoff.runtime,\n",
    "    predicate=lambda r: isinstance(r, AIOHTTPTransport),\n",
    "    value=lambda r: int(r.response_headers[\"RateLimit-Reset\"]) + 1,\n",
    "    jitter=None,\n",
    ")\n",
    "async def updateArticleTimeAfterProcessing(articleId, date = None):\n",
    "    async with create_client() as client: \n",
    "        try: \n",
    "            if date is not None: \n",
    "              # Wait a bit, it seems there's a race condition.\n",
    "              res = client.execute(updatePageSavedDate, variable_values={\n",
    "                  'id': articleId,\n",
    "                  'date': date,\n",
    "              })\n",
    "              return await res\n",
    "\n",
    "        except Exception as e:\n",
    "          if (hasattr(e, 'code') and e.code == 429): \n",
    "            return session.transport\n",
    "          \n",
    "          # I don't know why this happens and I will figure it out later.\n",
    "          print(e)\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio \n",
    "\n",
    "BATCH_UPDATE_SIZE = 10 # Bad name, we save to the DB after 100 so that we have a stop point.\n",
    "PARALLEL_API_CALL_SIZE = 1;\n",
    "\n",
    "def row_to_article(row):  \n",
    "   return { \n",
    "      \"id\": row[0],\n",
    "      \"read\": bool(row[1]),\n",
    "      \"time_added\": row[2],\n",
    "      \"tags\": [tagIds[tag] for tag in json.loads(row[3]) if tag != ''],\n",
    "      \"href\": row[4]\n",
    "   }\n",
    "\n",
    "all_articles = [row_to_article(row) for row in cursor.execute('select * from articles').fetchall()]\n",
    "unsaved_articles = list(filter(lambda article: article['id'] is None, all_articles)) \n",
    "saved_articles = list(filter(lambda article: article['id'] is not None, all_articles)) \n",
    "\n",
    "saved_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is likely that your articles will take a long time to process. Due to this we have added a wait time, and then go through and update all the labels and archive them where appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 30 min, might be longer, or shorter. Not 100% sure. Comment this bit out to execute immediately.\n",
    "time.sleep(1800)\n",
    "\n",
    "all_articles = [row_to_article(row) for row in cursor.execute('select * from articles').fetchall()]\n",
    "saved_articles = list(filter(lambda article: article['id'] is not None, all_articles)) \n",
    "\n",
    "articleBatches = [saved_articles[i:i+BATCH_UPDATE_SIZE] for i in range(0, len(saved_articles), BATCH_UPDATE_SIZE)]\n",
    "for archive_batch in articleBatches: \n",
    "   archive_futures = [] \n",
    "   for article in archive_batch:\n",
    "      archive_futures.append(asyncio.ensure_future(updateArticleTimeAfterProcessing(article['id'], article['time_added'])))\n",
    "      if (article['read'] == True): \n",
    "         archive_futures.append(asyncio.ensure_future(archiveArticle(article['id'])))\n",
    "            \n",
    "   results = await asyncio.gather(*archive_futures);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
